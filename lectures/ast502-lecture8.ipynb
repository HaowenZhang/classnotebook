{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align:center\">Bayesian Model Selection</h1><br>\n",
    "<p style=\"text-align:center\">Yuhui Tang</p>\n",
    "<p style=\"text-align:center\">February 17th, 2020</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Which model to use?</h2>\n",
    "\n",
    "<p>In the previous section, we talked about estimation of parameters, where we worked on the assumption of a certain model. Here we explore how to select the model to use, and compare it with other potential ones.</p>\n",
    "\n",
    "- Odds ratio\n",
    "- Hypthosis testing\n",
    "- Occam's razor\n",
    "- Model selection\n",
    "- Knuth's Histograms\n",
    "    - Bayesian blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Odds Ratio</h2>\n",
    "\n",
    "To compare two models, $M_1$ and $M_2$, we compare their posterior probability using the odds ratio, shown here the ratio in favor of model $M_2$ over $M_1$:\n",
    "\n",
    "<p style=\"text-align:center\">$O_{21}\\equiv\\frac{p(M_2|D,I)}{p(M_1|D,I)}$</p>\n",
    "\n",
    "<p style=\"text-align:center\">$p(M|D,I)=\\frac{p(D|M,I)p(M|I)}{p(D|I)}$</p>\n",
    "\n",
    "<p> The posterior probability for model $M$ given data $D$ can be obtatined from posterior pdf $p(M,\\boldsymbol{\\theta}|D,I)$ using marginalization over the model paramter space spanned by $\\boldsymbol{\\theta}$</p>\n",
    "\n",
    "<p style=\"text-align:center\">$E(M)\\equiv p(D|M,I)=\\int p(D|M,\\boldsymbol{\\theta},I)p(\\boldsymbol{\\theta}|M,I) d\\boldsymbol{\\theta}$</p>\n",
    "\n",
    "This $E(M)$ is called the *marginal likelihood* for model $M$, quantifying the probability that data $D$ would be observed if model $M$ were the correct model. It is also known as \"evidence\" as well as *global likelihood*, and will be called both in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>$p(D|I)$ is hard to compute, but it cancels out, leaving </p>\n",
    "\n",
    "<p style=\"text-align:center\">$O_{21}=\\frac{E(M_2)p(M_2|I)}{E(M_1)p(M_1|I)}=B_{21}\\frac{p(M_2|I)}{p(M_1|I)}$</p>\n",
    "\n",
    "<p>where</p>\n",
    "\n",
    "<p style=\"text-align:center\">$B_{21}=\\frac{\\int p(D|M_2,\\boldsymbol{\\theta}_2,I)p(\\boldsymbol{\\theta}_2|M_2,I) d\\boldsymbol{\\theta}_2}{\\int p(D|M_1,\\boldsymbol{\\theta}_1,I)p(\\boldsymbol{\\theta}_1|M_1,I) d\\boldsymbol{\\theta}_1}$</p>\n",
    "\n",
    "is called the *Bayes factor* and represents the ratio of the global likelihoods.\n",
    "\n",
    "Jeffreys suggests 5 step scale for interpreting the odds ratio, where $O_{21}>10$ represents \"strong\" evidence in favor of $M_2$ ($M_2$ is ten times more probable than $M_1$), $O_{21}>100$ is \"decisive\" evidence, and when $O_{21}<3$, evidence is \"not worth more than a bare mention\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>Let us consider a case of coin toss, where we compare two hypothesis: $M_1$, where the coin has a known heads probability $b_*$ (prior of delta function, $\\delta(b-b_*)$), and $M_2$, the heads probability $b$ is unknown, with a uniform prior in the range 0-1. We toss the coin $N$ times, obtaining $k$ heads. Assuming equal prior probabilities for the two models, and data likelihood </p>\n",
    "\n",
    "<p style=\"text-align:center\">$p(k|b,N)=\\frac{N!}{k!(N-k)!}b^k(1-b)^{N-k}$</p>\n",
    "\n",
    "we get an odds ratio of \n",
    "\n",
    "<p style=\"text-align:center\">$O_{21}=\\int_{0}^{1} (\\frac{b}{b_*})^k(\\frac{1-b}{1-b_*})^{N-k} db$</p>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure 5.1](http://www.astroml.org/_images/fig_odds_ratio_coin_1.png)\n",
    "\n",
    "More data gives more distinction between the models, and the $O_{21}<1$ when $k=b_*N$, favoring the simpler ratio.\n",
    "\n",
    "To build strong evidence for $M_2$, over 157 tosses are needed. $N=1000$ gvies $O_{21}\\approx1/80$, approaching the \"decisive\" limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Bayesian Hypothesis Testing</h2>\n",
    "\n",
    "Take $M_2=\\overline{M_1}$ is complementary hypothesis to $M_1$ (i.e. $p(M_1)+p(M_2)=1$ ). Assuming equal priors, $p(M_1|I)=p(M_2|I)$, the odds ratio is\n",
    "\n",
    "<p style=\"text-align:center\">$O_{21}=B_{21}=\\frac{p(D|M_2)}{p(D|M_1)}$</p>\n",
    "\n",
    "Because $M_2$ is only a complementary hypothesis to $M_1$, it is impossible to compute $p(D|M_2)$. This means that wihtout an alternative hypothesis, we are unable to reject $M_1$. Because Bayesian approach is based on posterior rather than data likelihood, a hypothesis cannot be rejected without an alternative. Thus, in the previous example of coin toss, we offered an alternative hypothesis that the coin has an unknown probability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h4>Bayesian vs Classical</h4>\n",
    "\n",
    "Classical or frequentist statistics is based on the relative frequencies of events. We ask for the probability, or frequency, of a measurement given a null hypothesis. In this example, we would ask, should we reject the hypothesis that the coin is fair? It is assumed we know the probability of a given outcome from the null hypothesis, and we can reject the null hypothesis based on the probability we would get this value. Let's say we get $k=16$ heads from $N=20$ tosses. We get $k=16$ is $2.7\\sigma_k$ from $b_*=0.5$, and we can reject the null hypothesis that the coin is fair. \n",
    "\n",
    "In Bayesian statistics, however, we offer an alternative hypothesis ($M_2$) that the coin has an unknown heads probability. This probability can be estimated from the data ($b^0$), but we consider *all possible values* of $b^0$ when we compare the two hypotheses $M_1$ and $M_2$. In this case, the Bayesian odds ratio is ~10 in favor of unfair coin hypothesis ($M_2$).\n",
    "\n",
    "One is based on relative frequency of \"something occuring\", other is the degree of subjective belief in \"something occuring\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Occam's razor and Information Criteria</h2>\n",
    "\n",
    "Occam's razor refers to the principle of selecting the simplest model that is in fair agreement witht the data. This is naturally included in Bayesian model comparison by penalizing complex models with many free parameters. \n",
    "\n",
    "\n",
    "Consider model $M(\\boldsymbol{\\theta})$, and examine one of its parameters, $\\mu=\\theta_1$. We can assume its prior pdf is flat in the range $-\\Delta_{\\mu}/2<\\mu<\\Delta_{\\mu}/2$, and so $p(\\mu|I)=1/\\Delta_{mu}$. Also assume the data likelihood can be well described by a Gaussian with center on the value of $\\mu$ that maximizes the likelihood, $\\mu^0$, and a width $\\sigma_{\\mu}$.\n",
    "\n",
    "When the data are more informative than the prior, $\\sigma_{\\mu}\\ll\\Delta$, the integral of approximate data likelihood is proportional to the product of $\\sigma_{\\mu}$ and the maximum value of the data likelihood, say $L^0(M)\\equiv\\textrm{max}[p(D|M)]$. This gives the global likelihood for model $M$ as approximately \n",
    "\n",
    "<p style=\"text-align:center\">$E(M)\\approx\\sqrt{2\\pi}L^0(M)\\frac{\\sigma_{\\mu}}{\\Delta_{\\mu}}$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"text-align:center\">$E(M)\\approx\\sqrt{2\\pi}L^0(M)\\frac{\\sigma_{\\mu}}{\\Delta_{\\mu}}$\n",
    "\n",
    "Therefore, $E(M)\\ll L^0(M)$ when $\\sigma_{\\mu}\\ll\\Delta_{\\mu}$. Every model parameter constrained by the model carries a similar multiplicative penalty, $\\propto \\sigma/\\Delta$, for the Bayes factor. If the parameter is not constrained by the data, then there is no penalty. \n",
    "\n",
    "Therefore, when computing an odds ratio, an additional parameter can only be justified if the penalty is offset, either by an increase of the maximum value of likelihood, $L^0(M)$, or by the ratio of model probabilities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Back to coin toss example, the data likelihood for model $M_2$ is \n",
    "\n",
    "<p style=\"text-align:center\">$L(b|M_2)=C_{Nk}b^k(1-b)^{N-k}$, where $C_{Nk}$ is the binomial coefficient. </p>\n",
    "\n",
    "This gives a maximum at $b=b^0=k/N$ of \n",
    "\n",
    "<p style=\"text-align:center\">$L^0(M_2)=C_{Nk}(b^0)^k(1-b^0)^{N-k}$</p>\n",
    "\n",
    "Assuming flat prior, evidence for model $M_2$ is \n",
    "\n",
    "<p style=\"text-align:center\">$E(M_2)\\approx\\sqrt{2\\pi}L^0(M_2)\\sigma_b$, where $\\sigma_b=\\sqrt{b^0(1-b^0)/N}$.</p>\n",
    "\n",
    "For model M_1, the prior is a Dirac delta function, so the exact result is \n",
    "\n",
    "<p style=\"text-align:center\">$E(M_1)=C_{Nk}(b_*)^k(1-b_*)^{N-k}\\approx\\sqrt{2\\pi}L^0(M_2)\\sigma_b$</p>\n",
    "\n",
    "Giving an odds ratio \n",
    "\n",
    "<p style=\"text-align:center\">$O_{21}=\\frac{E(M_2)}{E(M_1)}\\approx\\sqrt{2\\pi}\\sigma_b(\\frac{b^0}{b_*})^k(\\frac{1-b^0}{1-b_*})^{N-k}$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Bayesian information criterion* (BIC, also known as the *Schwarz criterion*) attempts to simplify calculations of the odds ratio by making certain assumptions about the likelihood, such as Gaussianity of the posterior pdf. For a given model, the BIC is calculated as\n",
    "\n",
    "<p style=\"text-align:center\">$\\textrm{BIC}\\equiv -2\\ln{(L^0(M))}+k\\ln{N}$</p>\n",
    "\n",
    "where $k$ is the number of model parameters and $N$ is the number of data points. The model with smaller BIC wins.\n",
    "\n",
    "This is similar to Akaike information criterion (AIC), which is used in classical statistics to compare models. The AIC is computed as \n",
    "\n",
    "<p style=\"text-align:center\">$\\textrm{AIC}\\equiv -2\\ln{(L^0(M))}+2k+\\frac{2k(k+1)}{N-k-1}$.</p>\n",
    "\n",
    "Comparing the two, BIC gives stronger penalties for additional model parameters than AIC when N is sufficiently large; very complex models are penalized more severely by BIC than AIC as well.\n",
    "\n",
    "Both are approximations, and may not be valid if underlying assumptions are not met. When possible, it is advisable to calculate the more exact odds ratio for comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Model selection</h2>\n",
    "\n",
    "Recall last class, where we looked at the posterior probability distribution function for a Gaussian and Cauchy distribution. In both cases, we had assumed such a model to be accurate. However, what if we wanted to test whether one model is better supported?\n",
    "\n",
    "![Figure 5.10](http://www.astroml.org/_images/fig_likelihood_cauchy_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assuming the same data as those used previously ($N=10$, $\\mu=0$, and $\\gamma=2$), we can obtain the model evidence by integrating the product of the data likelihood and the prior pdf for the model parameters \n",
    "\n",
    "<p style=\"text-align:center\">$E(M=\\textrm{Cauchy})=\\int p(\\{x_i\\}|\\mu,\\gamma,I)p(\\mu,\\gamma|I) d\\mu d\\gamma = 1.18\\times10^{-12}$</p>\n",
    "\n",
    "Using the same data set but now compute a Gaussian posterior pdf, we get\n",
    "\n",
    "<p style=\"text-align:center\">$E(M=\\textrm{Gaussian})=\\int p(\\{x_i\\}|\\mu,\\sigma,I)p(\\mu,\\sigma|I) d\\mu d\\sigma = 8.09\\times10^{-13}$</p>\n",
    "\n",
    "since no other information are available to prefer one model as oppose to the other, $p(M_C|I)=p(M_G|I)$, and so the odds ratio for Cauchy vs Gaussian is simply the Bayes factor,\n",
    "\n",
    "<p style=\"text-align:center\">$O_{CG}=\\frac{1.18\\times10^{-12}}{8.09\\times10^{-13}}=1.459$</p>\n",
    "\n",
    "This is very close to unity, showing an inconclusive result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, the data used in the last slide was only for $n=10$, which has relatively small chance of a data point far from the mean, which would argue strongly against the Gaussian model. Instead, as the number of data values increases, the ability to distinguish between the models will also increase. \n",
    "\n",
    "![Figure 5.19](http://www.astroml.org/_images/fig_odds_ratio_cauchy_1.png)\n",
    "\n",
    "Note the effect that the 36th point has on the odds ratio $O_{CG}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Knuth's Histograms</h2>\n",
    "\n",
    "We had previously examined histograms and the Scott's rule and Freedman-Diaconis rule for estimating optimal bin width. Both are undesirable for giving the same answer for both multimodal and unimodal distribution as long as their data size and scale parameter are the same. This can be avoided when using a Knuth's histograms. The best histogram has number of bins, $M$, which maximizes the function \n",
    "\n",
    "<p style=\"text-align:center\">$$F(M|{x_i},I)=N\\log{M} + \\log{[\\Gamma(\\frac{M}{2})]} - M\\log{[\\Gamma(\\frac{1}{2})]} - \\log{[\\Gamma(N+\\frac{M}{2})]} + \\sum_{k=1}^{M} \\log{[\\Gamma(n_k+\\frac{1}{2})]}$$</p>\n",
    "\n",
    "where $\\Gamma$ is the gamma function, $n_k$ is the number of measurements $x_i,i=1,...,N$, which are found in bin $k,k=1,...,M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is derived through Bayesian methods by estimating the histogram as a model of the data, where bin width is constant and the number of bins is the result of model selection. This creates a pdf of \n",
    "\n",
    "<p style=\"text-align:center\">$$h(x)=\\sum_{k=1}^{M} h_k\\Pi(x|x_{k-1},x_k),$$</p>\n",
    "\n",
    "where $\\Pi$ is boxcar fuction, $\\Pi=1$ if $x_{k-1}<x\\leq x_k$ and 0 otherwise. Because of normalization constraints, there are only $M-1$ free parameters. The uninformative prior distribution for ${h_k}$ is given by \n",
    "\n",
    "<p style=\"text-align:center\">$$p({h_k}|M,I)=\\frac{\\Gamma(\\frac{M}{2})}{\\Gamma(\\frac{1}{2})^M}[h_1h_2...h_{M-1}(1-\\sum_{k=1}^{M-1} h_k)]^{-1/2},$$</p>\n",
    "\n",
    "which is known as the *Jeffreys prior* for the multinomail likelihood. The joint data likelihood is a multinomial distribution \n",
    "\n",
    "<p style=\"text-align:center\">$$p({x_i}|{h_k},M,I)\\propto h_1^{n_1}h_2^{n_2}...h_M^{n_M}$$</p>\n",
    "\n",
    "The posterior pdf for the model parameters $h_k$ is obtained by multiplying the prior and data likelihood. The posterior probability for the number of bins $M$ is obtained by marginalizing the posterior pdf over all $h_k$. The logarithm of model posterior probability is the equation on the previous slide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a Gaussian distribution, numerical simulations show that $\\Delta_b=\\frac{2.7\\sigma_G}{N^1/4}$. While simple and applicable to non-Gaussian distributions if they are relatively simple, it is best to use the full equation (as shown below)\n",
    "\n",
    "![Figure 5.20](http://www.astroml.org/_images/fig_hist_binsize_1.png)\n",
    "\n",
    "Knuth's analysis is also capable of recognizing substructure in data, giving $M=1$ for uniform distribution and more bins for  multimodal than unimodal distribution, even when their data size and scale parameter are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Bayesian blocks</h3>\n",
    "\n",
    "Knuth's method can be further improved by introducing the block width as an additional model parameter instead of being constant. using Bayesian blocks formalism, the data are segmented into *blocks*, with the borders between two blocks being set by *changepoints*. Using a Bayesian analysis based on Pooissonian statistics within each block, we can define for each block an objective function, called the *log-likelihood fitness function*, \n",
    "\n",
    "<p style=\"text-align:center\">$F(N_i,T_i)=N_i(\\log{N_i}-\\log{T_i})$</p>\n",
    "\n",
    "where $N_i$ is the number of points in block $i$ and $T_i$ is the width of block $i$. The fitness function of a set of blocks is the sum of the fitness functions for each individual block. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure 5.20](http://www.astroml.org/_images/fig_bayes_blocks_1.png)\n",
    "\n",
    "The adaptive bin width of Bayesian blocks provide better representation of the data, especially of the narrow peaks at fewer data set sizes. The bins here are quantitatively optimal, which means there is statistical significance for the bin configuration. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
